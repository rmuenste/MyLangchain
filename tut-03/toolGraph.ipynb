{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3ebc411-1fd4-4ac4-b0e0-2430e3d797a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "#https://github.com/langchain-ai/langgraph/blob/main/examples/rag/langgraph_agentic_rag.ipynb\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "import pyowm\n",
    "from langchain_community.utilities import OpenWeatherMapAPIWrapper\n",
    "from langgraph.graph import Graph\n",
    "from langgraph.graph.message import add_messages \n",
    "from langchain_core.messages import AIMessage, HumanMessage, BaseMessage\n",
    "from typing import TypedDict, Annotated, Sequence\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langchain_core.utils.function_calling import convert_to_openai_function\n",
    "from langchain_community.tools.openweathermap import OpenWeatherMapQueryRun\n",
    "from langchain_core.utils.function_calling import convert_to_openai_function\n",
    "from langgraph.prebuilt import ToolInvocation\n",
    "import json\n",
    "from langchain_core.messages import FunctionMessage\n",
    "from langgraph.prebuilt import ToolExecutor\n",
    "# Now you can access your environment variables using os.environ\n",
    "\n",
    "load_dotenv(dotenv_path=\"../.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a4908e5-9e99-4ad7-82e2-96df44b64719",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the model as ChatOpenAI\n",
    "model = ChatOpenAI(temperature=0.5) \n",
    "\n",
    "weather = OpenWeatherMapAPIWrapper()\n",
    "\n",
    "tools = [OpenWeatherMapQueryRun()]\n",
    "\n",
    "functions = [convert_to_openai_function(function) for function in tools]\n",
    "\n",
    "model = model.bind_functions(functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23c43595-4bb7-4b30-848b-2259f9e2131f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1204827/2571114034.py:4: LangGraphDeprecationWarning: ToolExecutor is deprecated as of version 0.2.0 and will be removed in 0.3.0. Use langgraph.prebuilt.ToolNode instead.\n",
      "  tool_executor = ToolExecutor(tools)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]\n",
    "\n",
    "tool_executor = ToolExecutor(tools)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1788f62b-3c81-4d11-843e-f4215b88909f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_1(state):\n",
    "    messages = state[\"messages\"]\n",
    "    #user_input = messages[-1].content\n",
    "    response = model.invoke(messages)\n",
    "    print(f\"Node1 response: {response}\")\n",
    "    return {\"messages\": [response]}    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "85ed1a55-efd3-4ad7-a6fe-42c3990b5de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_2(state):\n",
    "    messages = state['messages']\n",
    "    last_message = messages[-1] # this has the query we need to send to the tool provided by the agent\n",
    "\n",
    "    parsed_tool_input = json.loads(last_message.additional_kwargs[\"function_call\"][\"arguments\"])\n",
    "    print(f\"Parsed tool input: {parsed_tool_input}\")\n",
    "\n",
    "    # We construct an ToolInvocation from the function_call and pass in the tool name and the expected str input for OpenWeatherMap tool\n",
    "    action = ToolInvocation(\n",
    "        tool=last_message.additional_kwargs[\"function_call\"][\"name\"],\n",
    "        tool_input=parsed_tool_input['location'],\n",
    "    )\n",
    "    \n",
    "    # We call the tool_executor and get back a response\n",
    "    response = tool_executor.invoke(action)\n",
    "\n",
    "    # We use the response to create a FunctionMessage\n",
    "    function_message = FunctionMessage(content=str(response), name=action.tool)\n",
    "\n",
    "    # We return a list, because this will get added to the existing list\n",
    "    return {\"messages\": [function_message]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "47987ceb-323e-4954-b5b0-0566170d5b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def where_to_go(state):\n",
    "    messages = state['messages']\n",
    "    last_message = messages[-1]\n",
    "    \n",
    "    if \"function_call\" in last_message.additional_kwargs:\n",
    "        return \"continue\"\n",
    "    else:\n",
    "        return \"end\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c00bfbfa-d485-4635-a51c-d0ee05611b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or you could import StateGraph and pass AgentState to it\n",
    "from langgraph.graph import StateGraph, END\n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "workflow.add_node(\"agent\", function_1)\n",
    "workflow.add_node(\"tool\", function_2)\n",
    "\n",
    "# The conditional edge requires the following info below.\n",
    "# First, we define the start node. We use `agent`.\n",
    "# This means these are the edges taken after the `agent` node is called.\n",
    "# Next, we pass in the function that will determine which node is called next, in our case where_to_go().\n",
    "\n",
    "workflow.add_conditional_edges(\"agent\", where_to_go,{   # Based on the return from where_to_go\n",
    "                                                        # If return is \"continue\" then we call the tool node.\n",
    "                                                        \"continue\": \"tool\",\n",
    "                                                        # Otherwise we finish. END is a special node marking that the graph should finish.\n",
    "                                                        \"end\": END\n",
    "                                                    }\n",
    ")\n",
    "\n",
    "# We now add a normal edge from `tools` to `agent`.\n",
    "# This means that if `tool` is called, then it has to call the 'agent' next. \n",
    "workflow.add_edge('tool', 'agent')\n",
    "\n",
    "# Basically, agent node has the option to call a tool node based on a condition, \n",
    "# whereas tool node must call the agent in all cases based on this setup.\n",
    "\n",
    "workflow.set_entry_point(\"agent\")\n",
    "\n",
    "\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bff09204-3f65-4b8c-9f49-a8d085001423",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node1 response: content='' additional_kwargs={'function_call': {'arguments': '{\"location\":\"Las Vegas\"}', 'name': 'open_weather_map'}, 'refusal': None} response_metadata={'token_usage': {'completion_tokens': 16, 'prompt_tokens': 79, 'total_tokens': 95}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'function_call', 'logprobs': None} id='run-b821a752-25c1-40a3-9c21-05f3744e0b1a-0' usage_metadata={'input_tokens': 79, 'output_tokens': 16, 'total_tokens': 95}\n",
      "Output from node 'agent':\n",
      "---\n",
      "{'messages': [AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\"location\":\"Las Vegas\"}', 'name': 'open_weather_map'}, 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 16, 'prompt_tokens': 79, 'total_tokens': 95}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'function_call', 'logprobs': None}, id='run-b821a752-25c1-40a3-9c21-05f3744e0b1a-0', usage_metadata={'input_tokens': 79, 'output_tokens': 16, 'total_tokens': 95})]}\n",
      "\n",
      "---\n",
      "\n",
      "Parsed tool input: {'location': 'Las Vegas'}\n",
      "Output from node 'tool':\n",
      "---\n",
      "{'messages': [FunctionMessage(content='In Las Vegas, the current weather is as follows:\\nDetailed status: few clouds\\nWind speed: 0.89 m/s, direction: 0°\\nHumidity: 25%\\nTemperature: \\n  - Current: 28.47°C\\n  - High: 30.42°C\\n  - Low: 27.36°C\\n  - Feels like: 27.23°C\\nRain: {}\\nHeat index: None\\nCloud cover: 24%', name='open_weather_map')]}\n",
      "\n",
      "---\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1204827/766937530.py:9: LangGraphDeprecationWarning: ToolInvocation is deprecated as of version 0.2.0 and will be removed in 0.3.0. Use langgraph.prebuilt.ToolNode instead.\n",
      "  action = ToolInvocation(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node1 response: content='The current temperature in Las Vegas is 28.47°C.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 14, 'prompt_tokens': 202, 'total_tokens': 216}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-60d1a06f-16c2-43a6-94ae-54e2d9422c1f-0' usage_metadata={'input_tokens': 202, 'output_tokens': 14, 'total_tokens': 216}\n",
      "Output from node 'agent':\n",
      "---\n",
      "{'messages': [AIMessage(content='The current temperature in Las Vegas is 28.47°C.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 14, 'prompt_tokens': 202, 'total_tokens': 216}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-60d1a06f-16c2-43a6-94ae-54e2d9422c1f-0', usage_metadata={'input_tokens': 202, 'output_tokens': 14, 'total_tokens': 216})]}\n",
      "\n",
      "---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inputs = {\"messages\": [HumanMessage(content=\"what is the temperature in las vegas\")]}\n",
    "#app.invoke(inputs)\n",
    "inputs = {\"messages\": [HumanMessage(content=\"what is the temperature in las vegas\")]}\n",
    "for output in app.stream(inputs):\n",
    "    # stream() yields dictionaries with output keyed by node name\n",
    "    for key, value in output.items():\n",
    "        print(f\"Output from node '{key}':\")\n",
    "        print(\"---\")\n",
    "        print(value)\n",
    "    print(\"\\n---\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913ea464-5635-4403-b992-70e7e5a7fba2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
